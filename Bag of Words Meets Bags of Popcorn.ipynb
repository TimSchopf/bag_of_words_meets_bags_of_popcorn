{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use different NLP methods to predict movie reviews\n",
    "\n",
    "### Adapted from the Kaggle 'Bag of Words Meets Bags of Popcorn' Competition\n",
    "\n",
    "For data and detailed description visit: https://www.kaggle.com/c/word2vec-nlp-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "#nltk.download() \n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( 'unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Text Preprocessing for Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML Markup and dealing with Punctuation, Numbers and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example BeatifulSoup object to remove HTML tags and markups\n",
    "example1 = BeautifulSoup(train['review'][0], 'lxml')\n",
    "example1.get_text()\n",
    "# Use regular expressions to do a find-and-replace for punctuations and numbers\n",
    "# [] indicates group membership and ^ means \"not\". In other words, the re.sub() statement says, \"Find anything that is NOT a lowercase letter (a-z) or an upper case letter (A-Z), and replace it with a space.\"\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words\n",
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do preprocessing for every review row\n",
    "train['review_preprocessed'] = train['review'].apply(lambda x: review_to_words(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "# choose only the 5000 most frequent words as features\n",
    "vectorizer = CountVectorizer(analyzer = 'word',tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(list(train['review_preprocessed']))\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each tuple (word in the vocabulary, count of the vocabulary word)\n",
    "# print the vocabulary word and the number of times it appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a random forest model with a bag of words representation and predict the sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "forest = RandomForestClassifier(n_estimators = 100).fit(X = train_data_features, y = train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do preprocessing for every review row in test set\n",
    "test['review_preprocessed'] = test['review'].apply(lambda x: review_to_words(x))\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(list(test['review_preprocessed']))\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv('Bag_of_Words_model.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Text Preprocessing for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting.\n",
    "\n",
    "To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentenceslist(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences and converts them\n",
    "    # to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, get a list of words of the sentence and append it to the sentences list\n",
    "            #\n",
    "            #  Remove HTML\n",
    "            review_text = BeautifulSoup(raw_sentence,'lxml').get_text()\n",
    "            #  \n",
    "            #  Remove non-letters\n",
    "            review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "            #\n",
    "            #  Convert words to lower case and split them\n",
    "            words = review_text.lower().split()\n",
    "            #\n",
    "            #  Optionally remove stop words (false by default)\n",
    "            if remove_stopwords:\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                words = [w for w in words if not w in stops]\n",
    "            sentences.append(words)\n",
    "    #\n",
    "    # 3. return the list of sentences (each sentence is a list of words, so this returns a list of lists\n",
    "    return(sentences)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# apply function to every review and create list of sentences, where each sentence is a list of words\n",
    "print('parse unlabeld train sentences')\n",
    "unlabeled_train['sentencelist'] = unlabeled_train['review'].apply(lambda x: review_to_sentenceslist(x,tokenizer=tokenizer, remove_stopwords=False))\n",
    "print('parse labeld train sentences')\n",
    "train['sentencelist'] = train['review'].apply(lambda x: review_to_sentenceslist(x,tokenizer=tokenizer, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to concatenate the lists of lists\n",
    "\n",
    "append adds an element to a list, and extend concatenates the first list with another list (or another iterable, not necessarily a list.)\n",
    "\n",
    "\n",
    "* append adds its argument as a single element to the end of a list. The length of the list itself will increase by one\n",
    "\n",
    "x = [1, 2, 3]  \n",
    "x.append([4, 5])  \n",
    "print (x)  \n",
    "\n",
    "result: [1, 2, 3, [4, 5]]\n",
    "\n",
    "* extend iterates over its argument adding each element to the list, extending the list. The length of the list will increase by however many elements were in the iterable argument\n",
    "\n",
    "x = [1, 2, 3]  \n",
    "x.extend([4, 5])  \n",
    "print (x)  \n",
    "\n",
    "result: [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list of sentences\n",
    "sentences = [] \n",
    "\n",
    "# extend sentences list to concatenate list of sentences from labeled and unlabeled train Dataframes\n",
    "print('extend sentences list with labeled training set sentences')\n",
    "for sentencelist in train['sentencelist']:\n",
    "    sentences.extend(sentencelist)\n",
    "\n",
    "print('extend sentences list with unlabeled training set sentences')\n",
    "for sentencelist in unlabeled_train['sentencelist']:\n",
    "    sentences.extend(sentencelist)\n",
    "print('number of sentences:',len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging to watch Word2Vec training flow\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train the Word2Vec Model\n",
    "print('train Word2Vec model...')\n",
    "word2vec_model = word2vec.Word2Vec(sentences=sentences, workers=4, size=300, min_count=40, window=10, sample=1e-3)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "# access its model.wv property, which holds the standalone keyed vectors\n",
    "word2vec_model = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to share word vector querying code between different training algos(Word2Vec, Fastext, WordRank, VarEmbed) Google separated storage and querying of word vectors into a separate class KeyedVectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = '300features_40minwords_10context_word2vec_model'\n",
    "word2vec_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "word2vec_model = KeyedVectors.load('300features_40minwords_10context_word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method is to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise). We add all up all values of the 300-dimensionl word vectors in one review and divide it by the number of words in the review. \n",
    "\n",
    "##### Example with a 2 word review:\n",
    "\n",
    "Vector1 = (0.1, 0.3, 0.5, ...) = word 1  \n",
    "Vector2 = (0.2, 0.5, 0.6, ...) = word 2  \n",
    "\n",
    "average_Vector = (Vector1 + Vector2)/2 = [(0.1, 0.3, 0.5, ...)+(0.2, 0.5, 0.6, ...)]/2 = (0.3, 0.8, 1.1, ...)/2  \n",
    "average_Vector = (0.15, 0.4, 0.55, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, vector_dimensionality):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((vector_dimensionality,),dtype='float32')\n",
    "    #\n",
    "    number_of_words = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            number_of_words = number_of_words + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,number_of_words)\n",
    "    return featureVec\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train and test reviews into words (stop word removal = True)\n",
    "\n",
    "print('split train reviews into words')\n",
    "train['words'] = train['review'].apply(lambda x: review_to_wordlist(x, remove_stopwords=True))\n",
    "print('split test reviews into words')\n",
    "test['words'] = test['review'].apply(lambda x: review_to_wordlist(x, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature vectors for train and test\n",
    "\n",
    "print('calculate average feature vector for train')\n",
    "train['average_feature_vector'] = train['words'].apply(lambda x: makeFeatureVec(words=x,model=word2vec_model,vector_dimensionality=word2vec_model.vector_size))\n",
    "print('calculate average feature vector for test')\n",
    "test['average_feature_vector'] = test['words'].apply(lambda x: makeFeatureVec(words=x,model=word2vec_model,vector_dimensionality=word2vec_model.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Model with the calculated average feature vector of reviews from the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert average feature vectors to 300-dimenesional numpy array for random forest classifier\n",
    "train_average_vectors = list(train['average_feature_vector'])\n",
    "train_average_vectors = np.asarray(train_average_vectors)\n",
    "print('shape of train array:',train_average_vectors.shape)\n",
    "\n",
    "test_average_vectors = list(test['average_feature_vector'])\n",
    "test_average_vectors = np.asarray(test_average_vectors)\n",
    "print('shape of test array:',test_average_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init random forest classifier \n",
    "word2vec_forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# fit model on train\n",
    "word2vec_forest = word2vec_forest.fit(train_average_vectors, train['sentiment'])\n",
    "\n",
    "# predict on test\n",
    "avg_features_result = word2vec_forest.predict(test_average_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the test results \n",
    "output2 = pd.DataFrame(data={'id':test['id'], 'sentiment':avg_features_result} )\n",
    "output2.to_csv('Word2Vec_AverageVectors.csv', index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words to Paragraphs, Attempt 2: Clustering \n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "num_clusters = round(word2vec_model.syn0.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "# The cluster assignment for each word is then stored in 'kmeans_cluster'\n",
    "kmeans_cluster = KMeans(n_clusters = num_clusters).fit_predict(word2vec_model.syn0)\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print('Time taken for K Means clustering:', elapsed, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word : Centroid dictionary, mapping each vocabulary word to a cluster number                                                                                \n",
    "word_centroid_dict = dict(zip(word2vec_model.index2word, kmeans_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert reviews into bags-of-centroids\n",
    "# This works just like Bag of Words but uses semantically related clusters instead of individual words\n",
    "\n",
    "def create_bag_of_centroids(wordlist, word_centroid_dict):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_dict.values()) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype='float32')\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_dict:\n",
    "            index = word_centroid_dict[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bags of centroids for our train and test\n",
    "\n",
    "print('calculate bags of centroids for train')\n",
    "train['bags_of_centroids'] = train['words'].apply(lambda x: create_bag_of_centroids(wordlist=x,word_centroid_dict=word_centroid_dict))\n",
    "print('calculate bags of centroids for test')\n",
    "test['bags_of_centroids'] = test['words'].apply(lambda x: create_bag_of_centroids(wordlist=x,word_centroid_dict=word_centroid_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Model with a bag of words representation of kmeans centroids of Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert average feature vectors to numpy array for random forest classifier\n",
    "train_bags_of_centroids = list(train['bags_of_centroids'])\n",
    "train_bags_of_centroids = np.asarray(train_bags_of_centroids)\n",
    "print('shape of train array:',train_bags_of_centroids.shape)\n",
    "\n",
    "test_bags_of_centroids = list(test['bags_of_centroids'])\n",
    "test_bags_of_centroids = np.asarray(test_bags_of_centroids)\n",
    "print('shape of test array:',test_bags_of_centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init random forest classifier \n",
    "bags_of_centroids_forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# fit model on train\n",
    "bags_of_centroids_forest = bags_of_centroids_forest.fit(train_bags_of_centroids, train['sentiment'])\n",
    "\n",
    "# predict on test\n",
    "bags_of_centroids_result = bags_of_centroids_forest.predict(test_bags_of_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the test results \n",
    "output3 = pd.DataFrame(data={'id':test['id'], 'sentiment':bags_of_centroids_result})\n",
    "output3.to_csv('BagOfCentroids.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

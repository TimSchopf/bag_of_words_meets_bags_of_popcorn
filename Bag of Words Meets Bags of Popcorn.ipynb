{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use different NLP methods to predict movie reviews\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "#nltk.download() \n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( 'unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Text Preprocessing for Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML Markup and dealing with Punctuation, Numbers and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example BeatifulSoup object to remove HTML tags and markups\n",
    "example1 = BeautifulSoup(train['review'][0], 'lxml')\n",
    "example1.get_text()\n",
    "# Use regular expressions to do a find-and-replace for punctuations and numbers\n",
    "# [] indicates group membership and ^ means \"not\". In other words, the re.sub() statement says, \"Find anything that is NOT a lowercase letter (a-z) or an upper case letter (A-Z), and replace it with a space.\"\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words\n",
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do preprocessing for every review row\n",
    "train['review_preprocessed'] = train['review'].apply(lambda x: review_to_words(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert text to bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "# choose only the 5000 most frequent words as features\n",
    "vectorizer = CountVectorizer(analyzer = 'word',tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(list(train['review_preprocessed']))\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each tuple (word in the vocabulary, count of the vocabulary word)\n",
    "# print the vocabulary word and the number of times it appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a random forest model and predict the sentiment with a bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "forest = RandomForestClassifier(n_estimators = 100).fit(X = train_data_features, y = train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do preprocessing for every review row in test set\n",
    "test['review_preprocessed'] = test['review'].apply(lambda x: review_to_words(x))\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(list(test['review_preprocessed']))\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv('Bag_of_Words_model.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Text Preprocessing for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists. For this reason, we'll use NLTK's punkt tokenizer for sentence splitting.\n",
    "\n",
    "To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentenceslist(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences and converts them\n",
    "    # to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, get a list of words of the sentence and append it to the sentences list\n",
    "            #\n",
    "            #  Remove HTML\n",
    "            review_text = BeautifulSoup(raw_sentence,'lxml').get_text()\n",
    "            #  \n",
    "            #  Remove non-letters\n",
    "            review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "            #\n",
    "            #  Convert words to lower case and split them\n",
    "            words = review_text.lower().split()\n",
    "            #\n",
    "            #  Optionally remove stop words (false by default)\n",
    "            if remove_stopwords:\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                words = [w for w in words if not w in stops]\n",
    "            sentences.append(words)\n",
    "    #\n",
    "    # 3. return the list of sentences (each sentence is a list of words, so this returns a list of lists\n",
    "    return(sentences)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# apply function to every review and create list of sentences, where each sentence is a list of words\n",
    "print('parse unlabeld train sentences')\n",
    "unlabeled_train['sentencelist'] = unlabeled_train['review'].apply(lambda x: review_to_sentenceslist(x,tokenizer=tokenizer, remove_stopwords=False))\n",
    "print('parse labeld train sentences')\n",
    "train['sentencelist'] = train['review'].apply(lambda x: review_to_sentenceslist(x,tokenizer=tokenizer, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to concatenate the list of lists\n",
    "\n",
    "append adds an element to a list, and extend concatenates the first list with another list (or another iterable, not necessarily a list.)\n",
    "\n",
    "\n",
    "* append adds its argument as a single element to the end of a list. The length of the list itself will increase by one\n",
    "\n",
    "x = [1, 2, 3]  \n",
    "x.append([4, 5])  \n",
    "print (x)  \n",
    "\n",
    "result: [1, 2, 3, [4, 5]]\n",
    "\n",
    "* extend iterates over its argument adding each element to the list, extending the list. The length of the list will increase by however many elements were in the iterable argument\n",
    "\n",
    "x = [1, 2, 3]  \n",
    "x.extend([4, 5])  \n",
    "print (x)  \n",
    "\n",
    "result: [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list of sentences\n",
    "sentences = [] \n",
    "\n",
    "# extend sentences list to concatenate list of sentences from labeled and unlabeled train Dataframes\n",
    "print('extend sentences list with labeled training set sentences')\n",
    "for sentencelist in train['sentencelist']:\n",
    "    sentences.extend(sentencelist)\n",
    "\n",
    "print('extend sentences list with unlabeled training set sentences')\n",
    "for sentencelist in unlabeled_train['sentencelist']:\n",
    "    sentences.extend(sentencelist)\n",
    "print('number of sentences:',len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging to watch Word2Vec flow\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train the Word2Vec Model\n",
    "print('train Word2Vec model...')\n",
    "word2vec_model = word2vec.Word2Vec(sentences=sentences, workers=4, size=300, min_count=40, window=10, sample=1e-3)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = '300features_40minwords_10context_word2vec_model'\n",
    "word2vec_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar('awful')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
